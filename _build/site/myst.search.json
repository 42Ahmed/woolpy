{"version":"1","records":[{"hierarchy":{"lvl1":"The Nature of Econometrics and Economic Data"},"type":"lvl1","url":"/chapter-1","position":0},{"hierarchy":{"lvl1":"The Nature of Econometrics and Economic Data"},"content":"import wooldridge\nimport pandas as pd\n\nwooldridge.data()\n\n","type":"content","url":"/chapter-1","position":1},{"hierarchy":{"lvl1":"The Nature of Econometrics and Economic Data","lvl2":"Computer problems"},"type":"lvl2","url":"/chapter-1#computer-problems","position":2},{"hierarchy":{"lvl1":"The Nature of Econometrics and Economic Data","lvl2":"Computer problems"},"content":"\n\n","type":"content","url":"/chapter-1#computer-problems","position":3},{"hierarchy":{"lvl1":"The Nature of Econometrics and Economic Data","lvl3":"C1","lvl2":"Computer problems"},"type":"lvl3","url":"/chapter-1#c1","position":4},{"hierarchy":{"lvl1":"The Nature of Econometrics and Economic Data","lvl3":"C1","lvl2":"Computer problems"},"content":"Use the data in Wage1 for this exercise\n\nwooldridge.data('wage1', description=True)\n\nwage = wooldridge.data('wage1')\nwage.head()\n\n(i) Find the average education level in the sample. What are the lowest and highest years of education?\n\nwage['educ'].describe()\n\nAverage years of education is 12.56, minimum years of education is 0 and maximum is 18\n\n(ii) Find the average hourly wage in the sample.\n\nwage['wage'].mean()\n\nThe wage in 1976 had an average 5.9\n\n(iii) How many women are in the sample? How many men?\n\nwage.columns\n\nwage.female.value_counts()\n\nThere are 274 males and 252 females in the dataset\n\n","type":"content","url":"/chapter-1#c1","position":5},{"hierarchy":{"lvl1":"The Nature of Econometrics and Economic Data","lvl3":"C2","lvl2":"Computer problems"},"type":"lvl3","url":"/chapter-1#c2","position":6},{"hierarchy":{"lvl1":"The Nature of Econometrics and Economic Data","lvl3":"C2","lvl2":"Computer problems"},"content":"Use the data in BWGHT to answer this question.\n\nwooldridge.data('bwght', description=True)\n\ndf2 = wooldridge.data('bwght')\ndf2.head()\n\n(i) How many women are in the sample, and how many report smoking during pregnancy?\n\nlen(df2)\n\ndf2[df2[\"cigs\"] != 0].shape[0]\n\ndf2[df2[\"cigs\"] != 0].shape[0] / len(df2)\n\nThere are 1388 women in the dataset, 212 of them reported smoking during pregnancy which repesents 15.3\\% of the women in the dataset\n\n(ii) What is the average number of cigarettes smoked per day? Is the average a good measure of the“typical” woman in this case? Explain.\n\ndf2[\"cigs\"].value_counts().sort_index().plot(kind=\"bar\")\n\ndf2[df2[\"cigs\"] != 0][\"cigs\"].value_counts().sort_index().plot(kind=\"bar\")\n\ndf2['cigs'].mean()\n\ndf2[df2['cigs'] != 0]['cigs'].mean()\n\nAverage ciggaretes per day is roughly 2 but its not representative cuz it is zero inflated. 1176 women did not smoke representing 85\\% of the data. If the zero inflation is removed, average becomes 13.7\n\n(iii) Among women who smoked during pregnancy, what is the average number of cigarettes\n\nThe average ciggaretes consumed by women who smoke during pregnancy is 13.7 ciggaretes per day\n\n(iv) Find the average of fatheduc in the sample. Why are only 1,192 observations used to compute this average?\n\ndf2['fatheduc'].value_counts().sort_index().plot(kind=\"bar\")\n\ndf2['fatheduc'].describe()\n\ndf2['fatheduc'].isnull().sum()\n\nThe average years of education of the fathers is 13.2. This average is based on 1192 father only cuz we have missing data for 196 fathers\n\n(v) Report the average family income and its standard deviation in dollars.\n\ndf2['faminc'].mean(), df2['faminc'].std()\n\nAverage family income is \\$ 29,027 with a standard deviation of \\$ 18,739\n\n","type":"content","url":"/chapter-1#c2","position":7},{"hierarchy":{"lvl1":"The Nature of Econometrics and Economic Data","lvl3":"C3","lvl2":"Computer problems"},"type":"lvl3","url":"/chapter-1#c3","position":8},{"hierarchy":{"lvl1":"The Nature of Econometrics and Economic Data","lvl3":"C3","lvl2":"Computer problems"},"content":"The data in MEAP01 are for the state of Michigan in the year 2001. Use these data to answer the following questions.\n\nwooldridge.data('meap01', description=True)\n\ndf3 = wooldridge.data('meap01')\n\n(i) Find the largest and smallest values of math4. Does the range make sense? Explain\n\ndf3['math4'].describe()\n\nThe smallest is 0 while the largest is 100. The range between smallest and largest values is massive.\n\n(ii) How many schools have a perfect pass rate on the math test? What percentage is this of the total sample?\n\ndf3['math4'].plot(kind=\"hist\", edgecolor='black', density=True, bins=10)\n\ndf3[df3['math4'] == 100]['math4'].count()\n\ndf3[df3['math4'] == 100]['math4'].count() / len(df3)\n\nlen(df3)\n\n38 out of 1823 schools have a perfect school representing 2.1\\% of the dataset\n\n(iii) How many schools have math pass rates of exactly 50%?\n\ndf3[df3['math4'] == 50]['math4'].count()\n\n17 schools\n\n(iv) Compare the average pass rates for the math and reading scores. Which test is harder to pass?\n\ndf3['math4'].mean(), df3['read4'].mean()\n\nBased on average pass rates, reading was harder to pass in 2001\n\n(v) Find the correlation between math4 and read4. What do you conclude?\n\nimport numpy as np\n\ndf3['math4'].corr(df3['read4'])\n\ndf3.plot(kind='scatter', x='math4', y='read4')\n\nThe sample correlation is 0.843 showing high linear association between the two tests. This indicates that schools that have high pass rates on one test is likely to have high pass rates for the other test\n\n(vi) The variable exppp is expenditure per pupil. Find the average of exppp along with its standard deviation. Would you say there is wide variation in per pupil spending?\n\ndf3['exppp'].plot(kind='hist', edgecolor='black', density=True)\n\ndf3['exppp'].mean().round(), df3['exppp'].std().round()\n\nThe mean is 5195 with a standard variation of 1092 showing high variability in the data\n\n(vii) Suppose School A spends 6,000 per student and School B spends 5,500 per student. By whatpercentage does School A’s spending exceed School B’s? Compare this to 100 · [log(6,000) –log(5,500)]\n\n\\begin{align*}\n\\text{Percentage difference} &= \\dfrac{\\text{A - B}}{\\text{B}}\\times 100 \\\\&= \\dfrac{6000-5500}{5500} \\times 100 \\\\&= 9\n\\\\ &= 100 \\times [\\log(6000) - \\log(5500)]\\\\\n&= 8.7\n\\end{align*}\n\n","type":"content","url":"/chapter-1#c3","position":9},{"hierarchy":{"lvl1":"The Nature of Econometrics and Economic Data","lvl3":"C4","lvl2":"Computer problems"},"type":"lvl3","url":"/chapter-1#c4","position":10},{"hierarchy":{"lvl1":"The Nature of Econometrics and Economic Data","lvl3":"C4","lvl2":"Computer problems"},"content":"The data in JTRAIN2 come from a job training experiment conducted for low-income men during 1976–1977; see Lalonde (1986).\n\nwooldridge.data('jtrain2', description=True)\n\ndf4 = wooldridge.data('jtrain2')\n\n(i) Use the indicator variable train to determine the fraction of men receiving job training.\n\ndf4['train'].value_counts()\n\n(df4['train'] == 1).mean()\n\n185  men recieved the training representing 41.6\\%\n\n(ii) The variable re78 is earnings from 1978, measured in thousands of 1982 dollars. Find the\naverages of re78 for the sample of men receiving job training and the sample not receiving job\ntraining. Is the difference economically large?\n\ndf4['re78'].groupby(df4['train']).mean().round(3)\n\nMen who recieved the training had average earnings of 6,349 while men who did not recieve the training had an average of 4,555\n\ntrain_0 = df4['re78'].groupby(df4['train']).mean()[0]\ntrain_1 = df4['re78'].groupby(df4['train']).mean()[1]\n\ntrain_0, train_1\n\n(train_1 - train_0) / train_0\n\nThose who trained has an earnings increase of 40\\% which is very large\n\n(iii) The variable unem78 is an indicator of whether a man is unemployed or not in 1978. What fraction of the men who received job training are unemployed? What about for men who did not receive job training? Comment on the difference.\n\ndf4['unem78'].groupby(df4['train']).value_counts()\n\ndf4['unem78'].groupby(df4['train']).mean().round(3)\n\n24.3\\% of those who trained were unemployed, while 35.4\\% of those who did noy train were unemployed. The difference in percentages seems large.\n\nFrom parts (ii) and (iii), does it appear that the job training program was effective? What would make our conclusions more convincing?\n\nThe difference in earnings and unemployment is large indicating that training had a positive effect.\nWe need to statistically test the difference to know if its significant or due to random error","type":"content","url":"/chapter-1#c4","position":11},{"hierarchy":{"lvl1":"The Simple Regression Model"},"type":"lvl1","url":"/chapter-2","position":0},{"hierarchy":{"lvl1":"The Simple Regression Model"},"content":"import pandas as pd\nimport numpy as np\nimport wooldridge\nimport statsmodels.formula.api as smf\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nwooldridge.data()\n\n","type":"content","url":"/chapter-2","position":1},{"hierarchy":{"lvl1":"The Simple Regression Model","lvl2":"Examples"},"type":"lvl2","url":"/chapter-2#examples","position":2},{"hierarchy":{"lvl1":"The Simple Regression Model","lvl2":"Examples"},"content":"\n\nceosal1 = wooldridge.data('ceosal1')\nwage1 = wooldridge.data('wage1')\nvote1 = wooldridge.data('vote1')\nmeap93 = wooldridge.data('meap93')\njtrain2 = wooldridge.data('jtrain2')\n\n","type":"content","url":"/chapter-2#examples","position":3},{"hierarchy":{"lvl1":"The Simple Regression Model","lvl3":"2.1 Soybean Yield and Fertilizer","lvl2":"Examples"},"type":"lvl3","url":"/chapter-2#id-2-1-soybean-yield-and-fertilizer","position":4},{"hierarchy":{"lvl1":"The Simple Regression Model","lvl3":"2.1 Soybean Yield and Fertilizer","lvl2":"Examples"},"content":"\n\nyield = \\beta_0 + \\beta_1 \\, fertilizer + u\n\n\\beta_1 is ceteris paribus effect of fertilizer on yieldu contains omitted factors\\Delta yield = \\beta_1 \\Delta fertilizer\n\n","type":"content","url":"/chapter-2#id-2-1-soybean-yield-and-fertilizer","position":5},{"hierarchy":{"lvl1":"The Simple Regression Model","lvl3":"2.2 A Simple Wage Equation","lvl2":"Examples"},"type":"lvl3","url":"/chapter-2#id-2-2-a-simple-wage-equation","position":6},{"hierarchy":{"lvl1":"The Simple Regression Model","lvl3":"2.2 A Simple Wage Equation","lvl2":"Examples"},"content":"\n\nwage = \\beta_0 + \\beta_1 \\, educ + u\n\n\\beta_1 measures change of wage associated with extra year of education, ceteris paribus\n\n","type":"content","url":"/chapter-2#id-2-2-a-simple-wage-equation","position":7},{"hierarchy":{"lvl1":"The Simple Regression Model","lvl3":"2.3 CEO Salary and Return on Equity","lvl2":"Examples"},"type":"lvl3","url":"/chapter-2#id-2-3-ceo-salary-and-return-on-equity","position":8},{"hierarchy":{"lvl1":"The Simple Regression Model","lvl3":"2.3 CEO Salary and Return on Equity","lvl2":"Examples"},"content":"\n\nwooldridge.data('ceosal1', description= True)\n\nmodel03 = smf.ols('salary ~ roe', data = ceosal1).fit()\nmodel03.params, model03.nobs\n\nSalary is \\beta_0 = $963,191 if return on equity is 0 [salary is measured in thousands]\n\nroe is defined of net income as a percentage of common equityIf return on equity increases by one percentage point, salary will increase by $18,500\n\nif roe=30, salary is expected to be (963.191 + 18.501(30)) * 1000 = 1,518,221\n\n","type":"content","url":"/chapter-2#id-2-3-ceo-salary-and-return-on-equity","position":9},{"hierarchy":{"lvl1":"The Simple Regression Model","lvl3":"2.4 Wage and Education","lvl2":"Examples"},"type":"lvl3","url":"/chapter-2#id-2-4-wage-and-education","position":10},{"hierarchy":{"lvl1":"The Simple Regression Model","lvl3":"2.4 Wage and Education","lvl2":"Examples"},"content":"\n\nwooldridge.data('wage1', description = True)\n\nmodel04 = smf.ols('wage~educ', data=wage1).fit()\nmodel04.params, model04.nobs\n\nwage1['educ'].plot(kind = 'hist')\n\nA person with no education is predicted to have an hourly wage of $-0.9. We got this result because the regression extrapolated\n\nOne more year of education is expected to increase predicted wage by $0.54\n\n","type":"content","url":"/chapter-2#id-2-4-wage-and-education","position":11},{"hierarchy":{"lvl1":"The Simple Regression Model","lvl3":"2.5 Voting Outcomes and Campaign Expenditures","lvl2":"Examples"},"type":"lvl3","url":"/chapter-2#id-2-5-voting-outcomes-and-campaign-expenditures","position":12},{"hierarchy":{"lvl1":"The Simple Regression Model","lvl3":"2.5 Voting Outcomes and Campaign Expenditures","lvl2":"Examples"},"content":"\n\nwooldridge.data('vote1', description = True)\n\nmodel05 = smf.ols('voteA ~ shareA', data = vote1).fit()\nmodel05.params, model05.nobs\n\nvoteA and shareA are both percentagesone more percentage point change in shareA results in almost half percentage point increase of total vote\n\n","type":"content","url":"/chapter-2#id-2-5-voting-outcomes-and-campaign-expenditures","position":13},{"hierarchy":{"lvl1":"The Simple Regression Model","lvl3":"2.6 CEO Salary and Return on Equity","lvl2":"Examples"},"type":"lvl3","url":"/chapter-2#id-2-6-ceo-salary-and-return-on-equity","position":14},{"hierarchy":{"lvl1":"The Simple Regression Model","lvl3":"2.6 CEO Salary and Return on Equity","lvl2":"Examples"},"content":"\n\ndf = wooldridge.data('ceosal1')\nmodel06 = smf.ols('salary ~ roe', data = df).fit()\n\ndf['salary_hat'] = model06.fittedvalues\ndf['u'] = model06.resid\ndf[['roe', 'salary', 'salary_hat', 'u']].head(10)\n\nint(df['u'].sum())\n\nThe \\beta's are chosen to make sum of residuals = 0\n\n","type":"content","url":"/chapter-2#id-2-6-ceo-salary-and-return-on-equity","position":15},{"hierarchy":{"lvl1":"The Simple Regression Model","lvl3":"2.7 Wage and Education","lvl2":"Examples"},"type":"lvl3","url":"/chapter-2#id-2-7-wage-and-education","position":16},{"hierarchy":{"lvl1":"The Simple Regression Model","lvl3":"2.7 Wage and Education","lvl2":"Examples"},"content":"\n\nmodel07 = smf.ols('wage~educ', data=wage1).fit()\nmodel07.params, model07.nobs\n\nround(wage1['educ'].mean(),2), round(wage1['wage'].mean(),2)\n\nnew_data = pd.DataFrame({'educ': [12.56]})\nmodel07.predict(new_data)\n\nRergerssion equation must pass through the mean points\n\n","type":"content","url":"/chapter-2#id-2-7-wage-and-education","position":17},{"hierarchy":{"lvl1":"The Simple Regression Model","lvl3":"2.8 CEO Salary and Return on Equity","lvl2":"Examples"},"type":"lvl3","url":"/chapter-2#id-2-8-ceo-salary-and-return-on-equity","position":18},{"hierarchy":{"lvl1":"The Simple Regression Model","lvl3":"2.8 CEO Salary and Return on Equity","lvl2":"Examples"},"content":"\n\nmodel08 = smf.ols('salary ~ roe', data = ceosal1).fit()\nmodel08.rsquared\n\nFirm return on equity explains 1.3% of the total variation in salary\n\n","type":"content","url":"/chapter-2#id-2-8-ceo-salary-and-return-on-equity","position":19},{"hierarchy":{"lvl1":"The Simple Regression Model","lvl3":"2.9 Voting Outcomes and Campaign Expenditures","lvl2":"Examples"},"type":"lvl3","url":"/chapter-2#id-2-9-voting-outcomes-and-campaign-expenditures","position":20},{"hierarchy":{"lvl1":"The Simple Regression Model","lvl3":"2.9 Voting Outcomes and Campaign Expenditures","lvl2":"Examples"},"content":"\n\nmodel09 = smf.ols('voteA ~ shareA', data = vote1).fit()\nmodel09.rsquared\n\nShare of campaign expenditures explains 85% of the variation in the election outcome\n\n","type":"content","url":"/chapter-2#id-2-9-voting-outcomes-and-campaign-expenditures","position":21},{"hierarchy":{"lvl1":"The Simple Regression Model","lvl3":"2.10 A log Wage Equation","lvl2":"Examples"},"type":"lvl3","url":"/chapter-2#id-2-10-a-log-wage-equation","position":22},{"hierarchy":{"lvl1":"The Simple Regression Model","lvl3":"2.10 A log Wage Equation","lvl2":"Examples"},"content":"\n\nmodel010 = smf.ols('np.log(wage)~educ', data=wage1).fit()\nmodel010.params, model010.nobs, model010.rsquared\n\nFor every year of education, wage increases by 0.083 * 100 = 8.3\\%\n\nNote: adding log allowed to get constant percentage change interpretation\n\n","type":"content","url":"/chapter-2#id-2-10-a-log-wage-equation","position":23},{"hierarchy":{"lvl1":"The Simple Regression Model","lvl3":"2.11 CEO Salary and Firm Sales","lvl2":"Examples"},"type":"lvl3","url":"/chapter-2#id-2-11-ceo-salary-and-firm-sales","position":24},{"hierarchy":{"lvl1":"The Simple Regression Model","lvl3":"2.11 CEO Salary and Firm Sales","lvl2":"Examples"},"content":"\n\nmodel011 = smf.ols('np.log(salary) ~ np.log(sales)', data = ceosal1).fit()\nmodel011.params\n\nA 1% increase in firm sales increases CEO salary by about 0.257% [elasticity model]\n\n","type":"content","url":"/chapter-2#id-2-11-ceo-salary-and-firm-sales","position":25},{"hierarchy":{"lvl1":"The Simple Regression Model","lvl3":"2.12 Student Math Performance and the School Lunch Program","lvl2":"Examples"},"type":"lvl3","url":"/chapter-2#id-2-12-student-math-performance-and-the-school-lunch-program","position":26},{"hierarchy":{"lvl1":"The Simple Regression Model","lvl3":"2.12 Student Math Performance and the School Lunch Program","lvl2":"Examples"},"content":"\n\nwooldridge.data('meap93', description = True)\n\nmodel012 = smf.ols('math10 ~ lnchprg', data = meap93).fit()\nmodel012.params\n\nIf student eligibility in lunch program increases by 10 percentage points, his math results falls by 3.2 percentage points\n\nNote: nonsensical data, which shows that error term is correlated with the independent variable\n\n","type":"content","url":"/chapter-2#id-2-12-student-math-performance-and-the-school-lunch-program","position":27},{"hierarchy":{"lvl1":"The Simple Regression Model","lvl3":"2.13 Heteroskedasticity in a Wage Equation","lvl2":"Examples"},"type":"lvl3","url":"/chapter-2#id-2-13-heteroskedasticity-in-a-wage-equation","position":28},{"hierarchy":{"lvl1":"The Simple Regression Model","lvl3":"2.13 Heteroskedasticity in a Wage Equation","lvl2":"Examples"},"content":"\n\nsns.lmplot(x='educ', y='wage', data=wage1, ci=None, line_kws={'color': 'red'})\n\n\nplt.title('Wage vs. Education (Check for Heteroscedasticity)')\nplt.xlabel('Years of Education')\nplt.ylabel('Wage')\nplt.show()\n\nas years of education increases, variability in wage increases\n\n","type":"content","url":"/chapter-2#id-2-13-heteroskedasticity-in-a-wage-equation","position":29},{"hierarchy":{"lvl1":"The Simple Regression Model","lvl3":"2.14 Evaluating a Job Training Program","lvl2":"Examples"},"type":"lvl3","url":"/chapter-2#id-2-14-evaluating-a-job-training-program","position":30},{"hierarchy":{"lvl1":"The Simple Regression Model","lvl3":"2.14 Evaluating a Job Training Program","lvl2":"Examples"},"content":"\n\nwooldridge.data('jtrain2', description = True)\n\nmodel014 = smf.ols('re78 ~ train', data = jtrain2).fit()\nmodel014.params, model014.rsquared\n\nWorkers who participated in the program earned $1790 more than those who did not, OLS assumptions are met due to random assignment\n\nR^2 = 1.8\\% shows that var(u) > var(y)\n\n","type":"content","url":"/chapter-2#id-2-14-evaluating-a-job-training-program","position":31},{"hierarchy":{"lvl1":"The Simple Regression Model","lvl2":"Computer Problems"},"type":"lvl2","url":"/chapter-2#computer-problems","position":32},{"hierarchy":{"lvl1":"The Simple Regression Model","lvl2":"Computer Problems"},"content":"\n\n","type":"content","url":"/chapter-2#computer-problems","position":33},{"hierarchy":{"lvl1":"The Simple Regression Model","lvl3":"C1","lvl2":"Computer Problems"},"type":"lvl3","url":"/chapter-2#c1","position":34},{"hierarchy":{"lvl1":"The Simple Regression Model","lvl3":"C1","lvl2":"Computer Problems"},"content":"The data in 401K\nare a subset of data analyzed by Papke (1995) to study the relationship between participation in a 401(k) pension plan and the generosity of the plan.\n\nThe variable prate is the percentage of eligible workers with an active account; this is the variable we would like to explain.\n\nThe measure of generosity is the plan match rate, mrate. This variable gives the average amount the firm\ncontributes to each worker’s plan for each $1 contribution by the worker.\n\nFor example, if mrate = 0.50, then a $1 contribution by the worker is matched by a 50¢ contribution by the firm\n\nwooldridge.data('401k', description=True)\n\ndf = wooldridge.data('401k')\ndf.head()\n\n(i) Find the average participation rate and the average match rate in the sample of plans\n\ndf['prate'].mean(), df['mrate'].mean()\n\nThe average prate is 87.36 and average mrate is 0.73\n\n(II) Now, estimate the simple regression equation and report the results along with the sample size and R-squared.\n\nmodel = smf.ols(formula='prate ~ mrate', data = df).fit()\n\nsummary_df = pd.DataFrame({\n    'term': model.params.index,\n    'coefficient': model.params.values\n})\nsummary_df['nobs'] = model.nobs\nsummary_df['rsquared'] = model.rsquared\n\nsummary_df\n\nThe intercept is 83.07, \\beta is 5.86, n = 1534, R^2 = 0.75\n\n(iii) Interpret the intercept in your equation. Interpret the coefficient on mrate.\n\nintercept = 83.75:- if mrate is zero, predicted participation rate is 83.05\nmrate = 5.86:- one dollar increase in mrate will cause prate to increase by 5.86 (given the prate range allows this increase)\n\n(iv) Find the predicted prate when mrate = 3.5. Is this a reasonable prediction? Explain what is happening here\n\nnew_data = pd.DataFrame({'mrate': [3.50]})\nmodel.predict(new_data)\n\nmodel.params['Intercept'] + model.params['mrate'] * 3.5\n\nif mrate = 3.5, prate = 103.5 which isn’t possible because prate is bounded by range 0 to 100\n\n(v) How much of the variation in prate is explained by mrate? Is this a lot in your opinion?\n\nmodel.rsquared * 100\n\nThe model explains 7.5% from the variation in prate, indicating the inlfuence of other factors on prate\n\n","type":"content","url":"/chapter-2#c1","position":35},{"hierarchy":{"lvl1":"The Simple Regression Model","lvl3":"C2","lvl2":"Computer Problems"},"type":"lvl3","url":"/chapter-2#c2","position":36},{"hierarchy":{"lvl1":"The Simple Regression Model","lvl3":"C2","lvl2":"Computer Problems"},"content":"The data set in CEOSAL2\ncontains information on chief executive officers for U.S. corporations. The\nvariable salary is annual compensation, in thousands of dollars, and ceoten is prior number of years as company CEO\n\nwooldridge.data('ceosal2', description= True)\n\ndf = wooldridge.data('ceosal2')\ndf.head()\n\n(i) Find the average salary and the average tenure in the sample\n\ndf['salary'].mean(), df['ceoten'].mean()\n\nAverage salary is $865,864. Average years is about 8 years\n\n(ii) How many CEOs are in their first year as CEO (that is, ceoten 5 0)? What is the longest tenure as a CEO?\n\n(df['ceoten'] == 0).sum(), df['ceoten'].max()\n\nThere are five CEO with ceoten = 0, Longest ceoten is 37\n\n(iii) Estimate the simple regression model \\log(\\text{salary}) = \\beta_0 + \\beta_1 \\text{ceoten} + u  and report your results in the usual form. What is the (approximate) predicted percentage increase in salary given one more year as a CEO?\n\nmodel = smf.ols(formula= 'np.log(salary) ~ ceoten', data=df).fit()\nmodel.params, model.nobs, model.rsquared\n\nOne more year of ceoten will increase salary by 0.0097 * 100 = 0.97\\%\n\n","type":"content","url":"/chapter-2#c2","position":37},{"hierarchy":{"lvl1":"The Simple Regression Model","lvl3":"C3","lvl2":"Computer Problems"},"type":"lvl3","url":"/chapter-2#c3","position":38},{"hierarchy":{"lvl1":"The Simple Regression Model","lvl3":"C3","lvl2":"Computer Problems"},"content":"Use the data in SLEEP75\nfrom Biddle and Hamermesh (1990) to study whether there is a tradeoff\nbetween the time spent sleeping per week and the time spent in paid work.\n\nWe could use either variable as the dependent variable. For concreteness, estimate the modelsleep = \\beta_0 + \\beta_1 totwrk + u\n\nwhere sleep is minutes spent sleeping at night per week and totwrk is total minutes worked during the week.\n\nwooldridge.data('sleep75', description= True)\n\ndf = wooldridge.data('sleep75')\ndf.head()\n\n(i) Report your results in equation form along with the number of observations and R2. What does the intercept in this equation mean?\n\nmodel = smf.ols(formula= 'sleep ~ totwrk', data = df).fit()\nmodel.params, model.nobs, model.rsquared\n\nIntercept is 3586.4:- if someone doesn’t work, he will sleep on average 3586.4 minutes per week, or 59.7 hours per week meaning 8.5 hours per day\n\ntotwrk = -0.15:- for every minute a person spends working, his sleeping time is reduced by 0.15 minute\n\n(ii) If totwrk increases by 2 hours, by how much is sleep estimated to fall? Do you find this to be a large effect?\n\nmodel.params['totwrk']*120\n\nIf a person works two more hours per week, his sleeping time will decrease by 18 minutes per week. Implying 2.57 minutes per day\n\n","type":"content","url":"/chapter-2#c3","position":39},{"hierarchy":{"lvl1":"The Simple Regression Model","lvl3":"","lvl2":"Computer Problems"},"type":"lvl3","url":"/chapter-2","position":40},{"hierarchy":{"lvl1":"The Simple Regression Model","lvl3":"","lvl2":"Computer Problems"},"content":"C4 Use the data in WAGE2\nto estimate a simple regression explaining monthly salary (wage) in terms of IQ score (IQ)\n\nwooldridge.data('wage2', description= True)\n\ndf = wooldridge.data('wage2')\ndf.head()\n\n(i) Find the average salary and average IQ in the sample. What is the sample standard deviation of IQ? (IQ scores are standardized so that the average in the population is 100 with a standard deviation equal to 15.)\n\ndf['wage'].mean(), df['IQ'].mean(), df['IQ'].std(ddof= 1)\n\nAverage salary is $957.95, Average IQ is 101.28. Sample standard deviation for IQ is 15.05\n\n(ii) Estimate a simple regression model where a one-point increase in IQ changes wage by a constant dollar amount. Use this model to find the predicted increase in wage for an increase in IQ of 15 points. Does IQ explain most of the variation in wage?\n\nmodel = smf.ols(formula= 'wage ~ IQ', data = df).fit()\nmodel.params, model.nobs, model.rsquared\n\nmodel.params['IQ']*15\n\nAccording to the model results, an increase of IQ by 15 points will result into increase of salary by $124.5\nIQ explains only 10% from the variation in wage\n\n(iii) Now, estimate a model where each one-point increase in IQ has the same percentage effect on wage. If IQ increases by 15 points, what is the approximate percentage increase in predicted wage?\n\nmodel = smf.ols(formula= 'np.log(wage) ~ IQ', data = df).fit()\nmodel.params, model.nobs, model.rsquared\n\nmodel.params['IQ']*15 * 100\n\nAn increase of IQ by 15 points will result into increase of salary by 13.2%","type":"content","url":"/chapter-2","position":41},{"hierarchy":{"lvl1":"Multiple Regression Analysis: Estimation"},"type":"lvl1","url":"/chapter-3","position":0},{"hierarchy":{"lvl1":"Multiple Regression Analysis: Estimation"},"content":"import pandas as pd\nimport numpy as np\nimport wooldridge\nimport statsmodels.formula.api as smf\n\ndef summarize_model(model):\n    \"\"\"\n    Returns a summary table with parameters, number of observations, and R-squared.\n\n    Parameters:\n        model: A fitted statsmodels regression result (e.g., OLS, Logit).\n\n    Returns:\n        pd.DataFrame: Summary table.\n    \"\"\"\n    params = model.params\n    summary_df = pd.DataFrame({\n        'Parameter': list(params.index) + ['Number of Observations', 'R-squared'],\n        'Estimate': list(params.values) + [int(model.nobs), model.rsquared]\n    })\n    return summary_df\n\n\nwooldridge.data()\n\n","type":"content","url":"/chapter-3","position":1},{"hierarchy":{"lvl1":"Multiple Regression Analysis: Estimation","lvl2":"Examples"},"type":"lvl2","url":"/chapter-3#examples","position":2},{"hierarchy":{"lvl1":"Multiple Regression Analysis: Estimation","lvl2":"Examples"},"content":"\n\n","type":"content","url":"/chapter-3#examples","position":3},{"hierarchy":{"lvl1":"Multiple Regression Analysis: Estimation","lvl3":"3.1 Determinants of College GPA","lvl2":"Examples"},"type":"lvl3","url":"/chapter-3#id-3-1-determinants-of-college-gpa","position":4},{"hierarchy":{"lvl1":"Multiple Regression Analysis: Estimation","lvl3":"3.1 Determinants of College GPA","lvl2":"Examples"},"content":"\n\nwooldridge.data(\"gpa1\", description=True)\n\ndf = wooldridge.data(\"gpa1\")\ndf.head()\n\nmodel01 = smf.ols(\"colGPA ~ hsGPA + ACT\", data=df).fit()\nmodel01.params, model01.nobs\n\n1.29 is the predicted college GPA is botg of hsGPA and ACT are zero, not meaningful cuz no one has zero high school GPA or a zero in ACT\n\nHolding ACT fixed, another point on hsGPA is associated with 0.45 point increase in colGPA\n\nHolding hsGPA fixed, a 10 unit change in ACT is associated with 0.094 change in colGPA, a very small effect\n\n","type":"content","url":"/chapter-3#id-3-1-determinants-of-college-gpa","position":5},{"hierarchy":{"lvl1":"Multiple Regression Analysis: Estimation","lvl3":"3.2 Hourly Wage Equation","lvl2":"Examples"},"type":"lvl3","url":"/chapter-3#id-3-2-hourly-wage-equation","position":6},{"hierarchy":{"lvl1":"Multiple Regression Analysis: Estimation","lvl3":"3.2 Hourly Wage Equation","lvl2":"Examples"},"content":"\n\nwooldridge.data(\"wage1\", description=True)\n\ndf = wooldridge.data(\"wage1\")\ndf.head()\n\nmodel02 = smf.ols(\"np.log(wage) ~ educ + exper + tenure\", data=df).fit()\nmodel02.params, model02.nobs\n\nHolding exper and tenure fixed, one more year of education is predicted to increase wage by 9.2\\%\n\n","type":"content","url":"/chapter-3#id-3-2-hourly-wage-equation","position":7},{"hierarchy":{"lvl1":"Multiple Regression Analysis: Estimation","lvl3":"3.3 Participation in 401(k) Pension Plans","lvl2":"Examples"},"type":"lvl3","url":"/chapter-3#id-3-3-participation-in-401-k-pension-plans","position":8},{"hierarchy":{"lvl1":"Multiple Regression Analysis: Estimation","lvl3":"3.3 Participation in 401(k) Pension Plans","lvl2":"Examples"},"content":"\n\nwooldridge.data(\"401k\", description=True)\n\ndf = wooldridge.data(\"401k\")\ndf.head()\n\nmodel03 = smf.ols(\"prate ~ mrate + age\", data=df).fit()\nmodel03.params, model03.nobs\n\nmodel032 = smf.ols(\"prate ~ mrate\", data=df).fit()\nmodel032.params, model032.nobs\n\ndf[\"age\"].corr(df[\"mrate\"])\n\nThe simple regression estimate of mrate effect is different from multiple regression, but the difference is not big due to the small correlation of 0.12\n\n","type":"content","url":"/chapter-3#id-3-3-participation-in-401-k-pension-plans","position":9},{"hierarchy":{"lvl1":"Multiple Regression Analysis: Estimation","lvl3":"3.4 Determinants of College GPA","lvl2":"Examples"},"type":"lvl3","url":"/chapter-3#id-3-4-determinants-of-college-gpa","position":10},{"hierarchy":{"lvl1":"Multiple Regression Analysis: Estimation","lvl3":"3.4 Determinants of College GPA","lvl2":"Examples"},"content":"\n\ndf = wooldridge.data(\"gpa1\")\n\nmodel04 = smf.ols(\"colGPA ~ hsGPA + ACT\", data=df).fit()\nsummarize_model(model04)\n\nR^2 = 17.6\\% means that hsGPA and ACT both explain around 17.6% of the variation in colGPA in the sample\n\n","type":"content","url":"/chapter-3#id-3-4-determinants-of-college-gpa","position":11},{"hierarchy":{"lvl1":"Multiple Regression Analysis: Estimation","lvl3":"3.5 Explaining Arrest Records","lvl2":"Examples"},"type":"lvl3","url":"/chapter-3#id-3-5-explaining-arrest-records","position":12},{"hierarchy":{"lvl1":"Multiple Regression Analysis: Estimation","lvl3":"3.5 Explaining Arrest Records","lvl2":"Examples"},"content":"\n\nwooldridge.data('crime1', description= True)\n\ndf = wooldridge.data('crime1')\ndf.head()\n\nmodel05 = smf.ols('narr86 ~ pcnv + ptime86 + qemp86', data = df).fit()\nsummarize_model(model05)\n\nmodel052 = smf.ols('narr86 ~ pcnv + ptime86 + qemp86 + avgsen', data = df).fit()\nsummarize_model(model052)\n\nAdding average sentence increases R^2 from 0.041 to 0.042 which is a small effect. And the sign is surprising.A longer Average sentence increases criminal activity\n\n","type":"content","url":"/chapter-3#id-3-5-explaining-arrest-records","position":13},{"hierarchy":{"lvl1":"Multiple Regression Analysis: Estimation","lvl3":"3.6 Hourly Wage Equation","lvl2":"Examples"},"type":"lvl3","url":"/chapter-3#id-3-6-hourly-wage-equation","position":14},{"hierarchy":{"lvl1":"Multiple Regression Analysis: Estimation","lvl3":"3.6 Hourly Wage Equation","lvl2":"Examples"},"content":"\n\ndf = wooldridge.data(\"wage1\")\ndf.head()\n\nmodel06 = smf.ols('np.log(wage) ~ educ', data =df).fit()\nsummarize_model(model06)\n\nability is an omitted variable that is positively correlated with education.Hence, the equation wage = \\beta_0 + \\beta_1 \\, educ + v is on average too large\n\nwe can’t say that 0.083 is greater than \\beta_1, it can be higher or lower than the true valueBut on average, the estimates from all random samples will be large\n\n","type":"content","url":"/chapter-3#id-3-6-hourly-wage-equation","position":15},{"hierarchy":{"lvl1":"Multiple Regression Analysis: Estimation","lvl3":"3.7 Evaluating a Job Training Program","lvl2":"Examples"},"type":"lvl3","url":"/chapter-3#id-3-7-evaluating-a-job-training-program","position":16},{"hierarchy":{"lvl1":"Multiple Regression Analysis: Estimation","lvl3":"3.7 Evaluating a Job Training Program","lvl2":"Examples"},"content":"\n\n","type":"content","url":"/chapter-3#id-3-7-evaluating-a-job-training-program","position":17},{"hierarchy":{"lvl1":"Multiple Regression Analysis: Estimation","lvl2":"Computer Problems"},"type":"lvl2","url":"/chapter-3#computer-problems","position":18},{"hierarchy":{"lvl1":"Multiple Regression Analysis: Estimation","lvl2":"Computer Problems"},"content":"\n\n","type":"content","url":"/chapter-3#computer-problems","position":19},{"hierarchy":{"lvl1":"Multiple Regression Analysis: Estimation","lvl3":"","lvl2":"Computer Problems"},"type":"lvl3","url":"/chapter-3","position":20},{"hierarchy":{"lvl1":"Multiple Regression Analysis: Estimation","lvl3":"","lvl2":"Computer Problems"},"content":"C1 A problem of interest to health officials (and others)\nis to determine the effects of smoking during pregnancy on infant health. One measure of infant health is birth weight;\n\na birth weight that is too low can put an infant at risk for contracting various illnesses. Since factors other than cigarette smoking that affect birth weight are likely to be correlated with smoking, we should take those factors into account.\n\nFor example, higher income generally results in access to better prenatal care, as well as better nutrition for the mother. An equation that recognizes this isbwght = \\beta_0 + \\beta_1 cigs + \\beta_2 faminc + u\n\n(i) What is the most likely sign for \\beta_2?\n\n\\beta_2 > 0 because more income is likely correlated with better nutirition\n\n(ii) Do you think cigs and faminc are likely to be correlated? Explain why the correlation might be positive or negative\n\nYes, negative correlation. As income increases, personal care and education will also increase, resulting in fewer ciggarates\n\n(iii) Now, estimate the equation with and without faminc, using the data in BWGHT. Report the results in equation form, including the sample size and R-squared. Discuss your results, focusing on whether adding faminc substantially changes the estimated effect of cigs on bwght\n\nwooldridge.data(\"bwght\", description=True)\n\ndf = wooldridge.data(\"bwght\")\ndf.head()\n\nmodel1 = smf.ols(formula=\"bwght ~ cigs\", data=df).fit()\nmodel1.params, model1.nobs, model1.rsquared\n\nmodel2 = smf.ols(formula=\"bwght ~ faminc + cigs\", data=df).fit()\nmodel2.params, model2.nobs, model2.rsquared\n\ndf[\"faminc\"].corr(df[\"cigs\"])\n\nThe correlation between faminc and cigs is weakly negative. When faminc is omitted, its positive effect on birth weight is absorbed into the cigs coefficient. Since faminc and cigs are negatively correlated, this leads to downward (negative) bias, making the effect of cigs appear slighly more harmful than it actually is.\n\n","type":"content","url":"/chapter-3","position":21},{"hierarchy":{"lvl1":"Multiple Regression Analysis: Estimation","lvl3":"C2","lvl2":"Computer Problems"},"type":"lvl3","url":"/chapter-3#c2","position":22},{"hierarchy":{"lvl1":"Multiple Regression Analysis: Estimation","lvl3":"C2","lvl2":"Computer Problems"},"content":"Use the data in HPRICE1 to estimate the model\\text{price} = \\beta_0 + \\beta_1 \\text{sqrft} + \\beta_2 \\text{bdrms} + u\n\nwooldridge.data(\"hprice1\", description=True)\n\ndf = wooldridge.data(\"hprice1\")\ndf.head()\n\n(i) Write out the results in equation form.\n\nmodel = smf.ols(formula=\"price ~ sqrft + bdrms\", data=df).fit()\nmodel.params, model.nobs, model.rsquared\n\n\\widehat{\\text{price}} = -19.32 + 0.128\\, \\text{sqrft} + 15.2\\, \\text{bdrms} \\\\\nn = 88,\\quad R^2 = 0.63\n\n(ii) What is the estimated increase in price for a house with one more bedroom, holding square footage constant?\n\nOne bedroom increase will increase price by $15,200 holding other factors fixed (price in equation is in thousands)\n\n(iii) What is the estimated increase in price for a house with an additional bedroom that is 140 square feet in size? Compare this to your answer in part (ii).\n\nmodel.params[\"sqrft\"] * 140 + model.params[\"bdrms\"] * 1\n\nAn additional room will result in home sqrft increasing leading to price increase of $33,179\n\n(iv) What percentage of the variation in price is explained by square footage and number of bedrooms?\n\nAbout 63.2%\n\n(v) The first house in the sample has sqrft = 2,438 and bdrms= 4. Find the predicted selling price for this house from the OLS regression line.\n\nmodel.predict(df.iloc[0])\n\nprint(\n    model.params[\"Intercept\"] + model.params[\"sqrft\"] * 2438 + model.params[\"bdrms\"] * 4\n)\n\nThe predicted price is $354,605\n\n(vi) The actual selling price of the first house in the sample was $300,000 (so price = 300). Find the residual for this house. Does it suggest that the buyer underpaid or overpaid for the house?\n\ndf.loc[0][\"price\"] - model.predict(df.iloc[0])\n\nResidual value is -54.605 which means that the buyer underpaid the house by $54,605\n\n","type":"content","url":"/chapter-3#c2","position":23},{"hierarchy":{"lvl1":"Multiple Regression Analysis: Estimation","lvl3":"C3","lvl2":"Computer Problems"},"type":"lvl3","url":"/chapter-3#c3","position":24},{"hierarchy":{"lvl1":"Multiple Regression Analysis: Estimation","lvl3":"C3","lvl2":"Computer Problems"},"content":"The file CEOSAL2\ncontains data on 177 chief executive officers and can be used to examine the effects of firm performance on CEO salary\n\nwooldridge.data(\"ceosal2\", description=True)\n\ndf = wooldridge.data(\"ceosal2\")\ndf.head()\n\n(i) Estimate a model relating annual salary to firm sales and market value. Make the model of the constant elasticity variety for both independent variables. Write the results out in equation form.\n\nmodel = smf.ols(\n    formula=\"np.log(salary) ~ np.log(sales) + np.log(mktval)\", data=df\n).fit()\nmodel.params, model.nobs, model.rsquared\n\n\\log(\\text{salary}) = 4.620917 + 0.162128 \\, \\log(\\text{sales}) + 0.106708 \\, \\log(\\text{mktval})\\\\\nn = 177 \\quad R^2 = 0.3\n\n(ii) Add profits to the model from part (i). Why can this variable not be included in logarithmic form? Would you say that these firm performance variables explain most of the variation in CEO salaries?\n\ndf[\"profits\"].describe()\n\nmodel2 = smf.ols(\n    formula=(\"np.log(salary) ~ np.log(sales) + \" \"np.log(mktval) + np.log(profits)\"),\n    data=df,\n).fit()\n\nmodel2.params, model2.nobs, model2.rsquared\n\nCan’t use  \\log(\\text{profit}) because it contains negative values\n\nmodel3 = smf.ols(\n    formula=(\"np.log(salary) ~ np.log(sales) + \" \"np.log(mktval) + profits\"), data=df\n).fit()\n\nmodel3.params, model3.nobs, model3.rsquared\n\nmodel3.params[\"profits\"] * 1000\n\nIf Profits increase by 1 billion, salary is expected to increase  approximately by 3.6%, ceteris paribus\n\nThe model explains around 30% of the sample variation in log sales which is not a lot.\n\n(iii) Add the variable ceoten to the model in part (ii). What is the estimated percentage return for another year of CEO tenure, holding other factors fixed?\n\nmodel4 = smf.ols(\n    formula=(\"np.log(salary) ~ np.log(sales) + \" \"np.log(mktval) + profits + ceoten\"),\n    data=df,\n).fit()\n\nmodel4.params, model4.nobs, model4.rsquared\n\nmodel4.params[\"ceoten\"] * 100\n\nOne additional year for a CEO is expected to increase salary by approximately 1.17%\n\n(iv) Find the sample correlation coefficient between the variables log(mktval) and profits. Are these variables highly correlated? What does this say about the OLS estimators?\n\ndf[\"profits\"].corr(np.log(df[\"mktval\"]))\n\nThe correlation is roughly 0.78, which indicates presence of multicollinearity.\nMulticollineairty does not bias the OLS esitmators but will increase the variance.\n\n","type":"content","url":"/chapter-3#c3","position":25},{"hierarchy":{"lvl1":"Multiple Regression Analysis: Estimation","lvl3":"C4","lvl2":"Computer Problems"},"type":"lvl3","url":"/chapter-3#c4","position":26},{"hierarchy":{"lvl1":"Multiple Regression Analysis: Estimation","lvl3":"C4","lvl2":"Computer Problems"},"content":"Use the data in ATTEND for this exercise\n\nwooldridge.data(\"attend\", description=True)\n\ndf = wooldridge.data(\"attend\")\ndf.head()\n\n(i) Obtain the minimum, maximum, and average values for the variables atndrte, priGPA, and ACT\n\ndf[[\"atndrte\", \"priGPA\", \"ACT\"]].agg([\"min\", \"max\", \"mean\"])\n\n(ii) Estimate the modelatndrte = \\beta_0 + \\beta_1 \\, priGPA + \\beta_2 \\, ACT + u\n\nand write the results in equation form. Interpret the intercept. Does it have a useful meaning?\n\nmodel = smf.ols(\"atndrte ~ priGPA + ACT\", data=df).fit()\nmodel.params, model.nobs, model.rsquared\n\natndrte = 75.7 + 17.3 \\, priGPA - 1.7 \\, ACT \\\\\nn = 680 \\qquad R^2 = 0.29\n\nThe intercept indicates that a student with a prior GPA of zero and an ACT score of zero is predicted to have an average attendance rate of 75.7%. However, this estimate is not reliable, as it is the result of extrapolation beyond the range of the observed data—no students in the sample have both a zero GPA and a zero ACT score.\n\n(iii) Discuss the estimated slope coefficients. Are there any surprises?\n\ndf[\"priGPA\"].corr(df[\"ACT\"])\n\nA one point increase in pri GPA increases attendace rate by 17.3 percentage points, holding ACT fixed.A one point increase in ACT score decreases attendace rate by 1.7 percentage points, holding prior GPA fixed. The negative relation may reflect that people with high ACT scores believe they can pass the lectures\n\n(iv) What is the predicted atndrte if priGPA = 3.65 and ACT = 20? What do you make of this result? Are there any students in the sample with these values of the explanatory variables?\n\nnew_data = pd.DataFrame({\"priGPA\": [3.65], \"ACT\": [20]})\nmodel.predict(new_data)\n\ntolerance = 0.1\n\ndf.loc[\n    (df[\"priGPA\"].sub(3.65).abs() <= tolerance) & (df[\"ACT\"].sub(20).abs() <= tolerance)\n]\n\nA student with prior GPA of 3.65 and ACT score of 20 is exptected to have 104% attendance rate.Since that attendance rate is bounded between zero and 100%, we predict 100% insteadThe actual student with this data has participation rate of 87.5%\n\n(v) If Student A has priGPA =  3.1 and ACT = 21 and Student B has priGPA = 2.1 and ACT = 26, what is the predicted difference in their attendance rates?\n\nnew_data = pd.DataFrame({\"priGPA\": [3.1, 2.1], \"ACT\": [21, 26]})\ny_hat = model.predict(new_data)\ny_hat[0] - y_hat[1]\n\nThe difference between the two predictions is around 25.84 percentage points","type":"content","url":"/chapter-3#c4","position":27},{"hierarchy":{"lvl1":"Multiple Regression Analysis: Inference"},"type":"lvl1","url":"/chapter-4","position":0},{"hierarchy":{"lvl1":"Multiple Regression Analysis: Inference"},"content":"import pandas as pd\nimport numpy as np\nimport wooldridge\nimport statsmodels.formula.api as smf\nimport matplotlib.pyplot as plt\n\nwooldridge.data()\n\n","type":"content","url":"/chapter-4","position":1},{"hierarchy":{"lvl1":"Multiple Regression Analysis: Inference","lvl2":"Examples"},"type":"lvl2","url":"/chapter-4#examples","position":2},{"hierarchy":{"lvl1":"Multiple Regression Analysis: Inference","lvl2":"Examples"},"content":"\n\nwage1 = wooldridge.data('wage1')\nmeap93 = wooldridge.data('meap93')\ngpa1 = wooldridge.data('gpa1')\ncampus = wooldridge.data('campus')\nhprice2 = wooldridge.data('hprice2')\nk401 = wooldridge.data('401k')\njtrain = wooldridge.data('jtrain')\nrdchem = wooldridge.data('rdchem')\nbwght = wooldridge.data('bwght')\n\n","type":"content","url":"/chapter-4#examples","position":3},{"hierarchy":{"lvl1":"Multiple Regression Analysis: Inference","lvl3":"4.1 Hourly Wage Equation","lvl2":"Examples"},"type":"lvl3","url":"/chapter-4#id-4-1-hourly-wage-equation","position":4},{"hierarchy":{"lvl1":"Multiple Regression Analysis: Inference","lvl3":"4.1 Hourly Wage Equation","lvl2":"Examples"},"content":"\n\nwooldridge.data('wage1', description= True)\n\nmodel01 = smf.ols('np.log(wage) ~ educ + exper + tenure', data = wage1).fit()\nmodel01.summary2().tables[1].iloc[:,:2]\n\nH_0: \\beta_{exper} = 0 \\qquad H_0: \\beta_{exper} > 0\n\nWe got exper has \\beta = 0.0041 with a standard error of 0.0017SO the t statistic is 0.0041/0.0017 = 2.41, t critical is 1.645 at 5% and 2.326 at 1%,t statistic > t critical  \\rightarrow we reject the null hypothesis at 1% significance level\n\n\\hat \\beta_{exper} is greater than zero\n\n","type":"content","url":"/chapter-4#id-4-1-hourly-wage-equation","position":5},{"hierarchy":{"lvl1":"Multiple Regression Analysis: Inference","lvl3":"4.2 Student Performance And School Size","lvl2":"Examples"},"type":"lvl3","url":"/chapter-4#id-4-2-student-performance-and-school-size","position":6},{"hierarchy":{"lvl1":"Multiple Regression Analysis: Inference","lvl3":"4.2 Student Performance And School Size","lvl2":"Examples"},"content":"\n\nwooldridge.data('meap93', description = True)\n\nmodel02 = smf.ols('math10 ~ totcomp + staff + enroll', data = meap93).fit()\nmodel02.summary2().tables[1].iloc[:,:2]\n\nH_0: \\beta_{enroll} = 0 \\qquad H_0: \\beta_{enroll} < 0\n\nIt is believed that smaller schools are associated with higher test scoresWe get \\beta = -0.0002 similar to the conjectureThe critical t is t =-1.65 at degree of freedoms n-k-1 = 408-4=404\nt statistic is 0.0002 / 0.00022 \\approx -0.91 \\rightarrow we fail to reject H_0school size is not statistically significant\n\nmodel022 = smf.ols('math10 ~ ltotcomp + lstaff + lenroll', data = meap93).fit()\nmodel022.summary2().tables[1].iloc[:,:2]\n\nIf we take level log form, t = -1.268/0.6932 \\approx -1.87 \\rightarrow we reject H_0If enrollment increases by 10%, math score is expected to decrease by 0.13 percentage points\n\nmodel02.rsquared, model022.rsquared\n\nlevel log model has higher R^2 so its better than level level model\n\n","type":"content","url":"/chapter-4#id-4-2-student-performance-and-school-size","position":7},{"hierarchy":{"lvl1":"Multiple Regression Analysis: Inference","lvl3":"4.3 Determinants of College GPA","lvl2":"Examples"},"type":"lvl3","url":"/chapter-4#id-4-3-determinants-of-college-gpa","position":8},{"hierarchy":{"lvl1":"Multiple Regression Analysis: Inference","lvl3":"4.3 Determinants of College GPA","lvl2":"Examples"},"content":"\n\nwooldridge.data('gpa1', description=True)\n\nmodel03 = smf.ols('colGPA ~ hsGPA + ACT + skipped', data = gpa1).fit()\nmodel03.summary2().tables[1].iloc[:, :2]\n\nH_0: \\beta = 0 \\qquad H_0: \\beta \\neq 0\n\nIn two sided alternatives, 5% critical value is 1.96 at df = 141-4=147 and 1% at 2.58hsGPA has t = 4.38 \\rightarrowACT has small coefficient and is practically and statistically insignificantskipped has t= -3.19 \\rightarrow statistically significant\n\n","type":"content","url":"/chapter-4#id-4-3-determinants-of-college-gpa","position":9},{"hierarchy":{"lvl1":"Multiple Regression Analysis: Inference","lvl3":"4.4 Campus Crime and Enrollment","lvl2":"Examples"},"type":"lvl3","url":"/chapter-4#id-4-4-campus-crime-and-enrollment","position":10},{"hierarchy":{"lvl1":"Multiple Regression Analysis: Inference","lvl3":"4.4 Campus Crime and Enrollment","lvl2":"Examples"},"content":"\n\nwooldridge.data('campus', description= True)\n\nmodel04 = smf.ols('lcrime ~ lenroll', data = campus).fit()\nmodel04.summary2().tables[1].iloc[:, :2]\n\nH_0: \\beta_1 =1 \\qquad H_1: \\beta_1 >1\n\nWe didn’t test for \\beta_1 = 0 cuz its expected, \\beta_1 =1 is more interesting, if \\beta_1 >1, then crime is a big problem on large campusescrime = \\exp(\\beta_0)enroll^{\\beta_1}\\exp(u)\n\nenroll = np.linspace(0.000001, 2, 100)\n\n\ncrime_elastic_05 = np.exp(0.5 * np.log(enroll))   # β = 0.5\ncrime_elastic_1  = np.exp(1.0 * np.log(enroll))   # β = 1.0\ncrime_elastic_15 = np.exp(1.5 * np.log(enroll))   # β = 1.5\n\nplt.figure(figsize=(10, 6))\nplt.plot(enroll, crime_elastic_05, label=r'$\\beta_1 = 0.5$', linestyle='--')\nplt.plot(enroll, crime_elastic_1,  label=r'$\\beta_1 = 1$', linestyle='-')\nplt.plot(enroll, crime_elastic_15, label=r'$\\beta_1 = 1.5$', linestyle=':')\n\nplt.xlabel('Enrollment')\nplt.ylabel('Crime')\nplt.title('Elasticity Models: log(Crime) ~ β * log(Enroll)')\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\nthe t statistic reported 0.11 is for testing \\beta = 0The t statistic we need is\\dfrac{1.27 - 1}{0.11} \\approx 2.45\n\nreject H_0\n\nmodel04.t_test(\"lenroll = 1\").tvalue[0]\n\n","type":"content","url":"/chapter-4#id-4-4-campus-crime-and-enrollment","position":11},{"hierarchy":{"lvl1":"Multiple Regression Analysis: Inference","lvl3":"4.5 Housing Prices and Air Pollution","lvl2":"Examples"},"type":"lvl3","url":"/chapter-4#id-4-5-housing-prices-and-air-pollution","position":12},{"hierarchy":{"lvl1":"Multiple Regression Analysis: Inference","lvl3":"4.5 Housing Prices and Air Pollution","lvl2":"Examples"},"content":"\n\nwooldridge.data('hprice2', description= True)\n\nmodel05 = smf.ols('lprice ~ lnox + np.log(dist) + rooms + stratio', data = hprice2).fit()\nmodel05.summary2().tables[1].iloc[:, :2]\n\nH_0: \\beta_1 = -1 \\qquad H_1: \\beta_1 \\neq -1\n\nt = \\frac{-0.954 + 1}{0.117} = 0.393\n\nt is so small, we fail to reject H_0\n\nmodel05.t_test(\"lnox = -1\").tvalue[0]\n\n","type":"content","url":"/chapter-4#id-4-5-housing-prices-and-air-pollution","position":13},{"hierarchy":{"lvl1":"Multiple Regression Analysis: Inference","lvl3":"4.6 Participation Rates in 401(k) Plans","lvl2":"Examples"},"type":"lvl3","url":"/chapter-4#id-4-6-participation-rates-in-401-k-plans","position":14},{"hierarchy":{"lvl1":"Multiple Regression Analysis: Inference","lvl3":"4.6 Participation Rates in 401(k) Plans","lvl2":"Examples"},"content":"\n\nwooldridge.data('401k', description= True)\n\nmodel06 = smf.ols('prate ~ mrate + age + totemp', data = k401).fit()\nmodel06.summary2().tables[1].iloc[:, :4]\n\nAll variables are statistically significant, but totemp has very small coefficient making it practically insignificant\n\n","type":"content","url":"/chapter-4#id-4-6-participation-rates-in-401-k-plans","position":15},{"hierarchy":{"lvl1":"Multiple Regression Analysis: Inference","lvl3":"4.7 Effect of Job Training On Firm Scrap Rates","lvl2":"Examples"},"type":"lvl3","url":"/chapter-4#id-4-7-effect-of-job-training-on-firm-scrap-rates","position":16},{"hierarchy":{"lvl1":"Multiple Regression Analysis: Inference","lvl3":"4.7 Effect of Job Training On Firm Scrap Rates","lvl2":"Examples"},"content":"\n\nwooldridge.data('jtrain', description= True)\n\ndf= jtrain[(jtrain['year'] == 1987) & (jtrain['union'] == 0)]\n\nmodel07 = smf.ols('np.log(scrap)~hrsemp + np.log(sales) + np.log(employ)', data = df).fit()\nmodel07.summary2().tables[1].iloc[:, :4]\n\nOne more hour of training per employee [hrsemp] lowers scrap by 2.9%t statistic is small, its not statistically significant\n\nmodel07.nobs\n\nlen(df)\n\ncols = ['scrap', 'hrsemp', 'sales', 'employ']\ndf[cols].dropna().shape[0]\n\nAlthough the data has 126 rows, only 29 rows have no missing data in the rows of columns we use in the regression\n\nfrom scipy import stats\n\ncoef = model07.params['hrsemp']\nse = model07.bse['hrsemp']\n\nt_stat = coef / se\n\n# One-sided p-value (lower tail)\nstats.t.cdf(t_stat, df=model07.df_resid)\n\nDue to the small sample size, we calculate p value for one sided test at 10% significance level and get 0.106, its almost rejected\n\n","type":"content","url":"/chapter-4#id-4-7-effect-of-job-training-on-firm-scrap-rates","position":17},{"hierarchy":{"lvl1":"Multiple Regression Analysis: Inference","lvl3":"4.8 Model of R&D Expenditures","lvl2":"Examples"},"type":"lvl3","url":"/chapter-4#id-4-8-model-of-r-d-expenditures","position":18},{"hierarchy":{"lvl1":"Multiple Regression Analysis: Inference","lvl3":"4.8 Model of R&D Expenditures","lvl2":"Examples"},"content":"\n\nwooldridge.data('rdchem', description= True)\n\nmodel08 = smf.ols('lrd ~ lsales + profmarg', data = rdchem).fit()\nmodel08.summary2().tables[1]\n\nHolding profit margin fixed, 1% increase in sales is associated with 1.084% increase in R&D spendingThe 95% Confidence interval is [0.96 - 1.21] which doesn’t include 0 leading to rejection of H_0But it includes the unity 1, so 1% increase in sales is associated with 1% increase in R&D spending\n\nThe 95% Confidence interval for profit margin is [-0.005, 0.05] which includes the zero, we fail to reject H_0 at the 5% significance level using a two-sided test\nHowever, using a one-sided alternative hypothesis H_1: \\beta_2 > 0, the one-sided p-value is 0.0504, making the coefficient statistically significant at the 5% level.This suggests that a 1-unit increase in profit margin, holding sales constant, is associated with a 2.2% increase in R&D spending, on average.\n\ncoef = model08.params['profmarg']\nse = model08.bse['profmarg']\n\nt_stat = coef / se\n\n# One-sided p-value (upper tail)\n1 - stats.t.cdf(t_stat, df=model08.df_resid)\n\n","type":"content","url":"/chapter-4#id-4-8-model-of-r-d-expenditures","position":19},{"hierarchy":{"lvl1":"Multiple Regression Analysis: Inference","lvl3":"4.9 Parents Education in a Birth Weight Equation","lvl2":"Examples"},"type":"lvl3","url":"/chapter-4#id-4-9-parents-education-in-a-birth-weight-equation","position":20},{"hierarchy":{"lvl1":"Multiple Regression Analysis: Inference","lvl3":"4.9 Parents Education in a Birth Weight Equation","lvl2":"Examples"},"content":"\n\nwooldridge.data('bwght', description= True)\n\ndf = bwght[['bwght', 'cigs', 'parity', 'faminc', 'motheduc', 'fatheduc']].dropna()\n\nmodel09 = smf.ols('bwght ~ cigs + parity + faminc + motheduc + fatheduc', data = df).fit()\nmodel09.summary2().tables[1]\n\nmodel09r = smf.ols('bwght ~ cigs + parity + faminc', data=df).fit()\nmodel09r.summary2().tables[1]\n\nR2_ur = model09.rsquared\nR2_r = model09r.rsquared\nn = model09.nobs              \nk_ur = model09.df_model + 1   \nq = 2 \n\nnumerator = (R2_ur - R2_r) / q\ndenominator = (1 - R2_ur) / (n - k_ur)\nF_stat = numerator / denominator\nF_stat\n\nH_0: \\beta_4 =0, \\beta_5 = 0\n\nTo test the joint significance of parents’ education, we must ensure both the unrestricted and restricted models use the same observations.\n\nTherefore, we first subset the data to exclude any rows with missing values in the unrestricted model. This ensures both models are estimated on the same sample, allowing a valid comparison.\n\nNote: If you use statsmodels’s built-in .f_test() function after fitting the unrestricted model, it automatically uses the correct sample. So you do not need to drop missing values manually in that case.\n\nmodel092 = smf.ols('bwght ~ cigs + parity + faminc + motheduc + fatheduc', data = bwght).fit()\nmodel092.f_test(\"motheduc = 0, fatheduc = 0\")\n\nWe fail to reject the null \n\nhypothesis.In other words, parents education is jointly insignificant\n\n","type":"content","url":"/chapter-4#id-4-9-parents-education-in-a-birth-weight-equation","position":21},{"hierarchy":{"lvl1":"Multiple Regression Analysis: Inference","lvl3":"4.10 Salary-Pension Tradeoff for Teachers","lvl2":"Examples"},"type":"lvl3","url":"/chapter-4#id-4-10-salary-pension-tradeoff-for-teachers","position":22},{"hierarchy":{"lvl1":"Multiple Regression Analysis: Inference","lvl3":"4.10 Salary-Pension Tradeoff for Teachers","lvl2":"Examples"},"content":"\n\nif totcomp [total compensation] includes annual salary of teacher and benefits [like insurance]thentotcomp = salary + benefits\n\nwhich can be rewritten astotcomp = salary \\left( 1 + \\dfrac{benefits}{salary} \\right)\n\ntake the log to get\\log(totcomp) = \\log(salary) + \\log \\left(1+ \\dfrac b s \\right)\n\nfor small \\dfrac b s, we have the approximation\\log(totcomp) = \\log(salary) + \\dfrac b s\n\nTo understandard salary-benefits tradeoffs, get the econometric model\\log(salary) = \\beta_0 + \\beta_1 \\dfrac b s + u\n\nTo test salary benefits tradeoff, test H_0: \\beta_1 = -1The simple regression fails to reject the null, but it becomes significant after adding controls\n\ndf = meap93.copy()\ndf['bs_ratio'] = df['benefits'] / df['salary']\nmodel010 = smf.ols('lsalary ~ bs_ratio', data=df).fit()\nmodel010.summary2().tables[1].iloc[:,:4]\n\nmodel010.t_test('bs_ratio = -1')\n\n","type":"content","url":"/chapter-4#id-4-10-salary-pension-tradeoff-for-teachers","position":23},{"hierarchy":{"lvl1":"Multiple Regression Analysis: Inference","lvl3":"4.11 Evaluating a Job Training Program","lvl2":"Examples"},"type":"lvl3","url":"/chapter-4#id-4-11-evaluating-a-job-training-program","position":24},{"hierarchy":{"lvl1":"Multiple Regression Analysis: Inference","lvl3":"4.11 Evaluating a Job Training Program","lvl2":"Examples"},"content":"\n\n","type":"content","url":"/chapter-4#id-4-11-evaluating-a-job-training-program","position":25},{"hierarchy":{"lvl1":"Multiple Regression Analysis: Inference","lvl2":"🚧 Computer Exercises"},"type":"lvl2","url":"/chapter-4#id-computer-exercises","position":26},{"hierarchy":{"lvl1":"Multiple Regression Analysis: Inference","lvl2":"🚧 Computer Exercises"},"content":"","type":"content","url":"/chapter-4#id-computer-exercises","position":27},{"hierarchy":{"lvl1":"Welcome to Introductory Econometrics with Python"},"type":"lvl1","url":"/chapter-0","position":0},{"hierarchy":{"lvl1":"Welcome to Introductory Econometrics with Python"},"content":"This book is a Python-based companion to Introductory Econometrics: A Modern Approach by Jeffrey M. Wooldridge.","type":"content","url":"/chapter-0","position":1},{"hierarchy":{"lvl1":"Welcome to Introductory Econometrics with Python","lvl2":"🔍 What you’ll find here"},"type":"lvl2","url":"/chapter-0#id-what-youll-find-here","position":2},{"hierarchy":{"lvl1":"Welcome to Introductory Econometrics with Python","lvl2":"🔍 What you’ll find here"},"content":"📘 Book Examples implemented in PythonStep-by-step walkthroughs of every example in the textbook, using pandas, statsmodels, and related libraries.\n\n💻 Computer Exercises with full Python codeAll end-of-chapter problems solved and visualized in notebooks and scripts.\n\n🧠 Theory meets PracticeClear links between econometric concepts and reproducible Python code.\n\n🎯 Self-contained notebooksEach chapter can be run independently with minimal setup.","type":"content","url":"/chapter-0#id-what-youll-find-here","position":3},{"hierarchy":{"lvl1":"Welcome to Introductory Econometrics with Python","lvl2":"👤 Author"},"type":"lvl2","url":"/chapter-0#id-author","position":4},{"hierarchy":{"lvl1":"Welcome to Introductory Econometrics with Python","lvl2":"👤 Author"},"content":"Ahmed H. DarwishApplied Statistician | Teaching Assistant | Open-source Contributor","type":"content","url":"/chapter-0#id-author","position":5},{"hierarchy":{"lvl1":"Welcome to Introductory Econometrics with Python","lvl2":"📚 Credits"},"type":"lvl2","url":"/chapter-0#id-credits","position":6},{"hierarchy":{"lvl1":"Welcome to Introductory Econometrics with Python","lvl2":"📚 Credits"},"content":"This project is inspired by the excellent textbook by \n\nJeffrey M. Wooldridge and built using \n\nMyST Markdown and \n\nJupyter Book.","type":"content","url":"/chapter-0#id-credits","position":7}]}